# -*- coding: utf-8 -*-
"""Lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18KJGZPPfyZnyMPaYfamQgZXzLDPNf0Ce

# –í—ã–±–æ—Ä —Å—Ü–µ–Ω–∞—Ä–∏—è –∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è

–î–∞—Ç–∞—Å–µ—Ç: –î–∞–Ω–Ω—ã–µ –æ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–±—Ä–µ–Ω–¥–æ–≤–æ–º –º–∞–≥–∞–∑–∏–Ω–µ

https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store

3. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø—Ä–æ—Å–∞

    –¶–µ–ª—å: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø—Ä–æ—Å–∞ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã

    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:

      *   –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –≥–æ—Ç–æ–≤ –∫ 6:00 —É—Ç—Ä–∞
      *   –¢–æ—á–Ω–æ—Å—Ç—å (MAPE) < 15%
      *   –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
      *   –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (6 –ø—É–Ω–∫—Ç)

# –ß–∞—Å—Ç—å 1: –ü—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
"""

# –ß–∞—Å—Ç—å 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫
print("=" * 50)
print("–ù–ê–°–¢–†–û–ô–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø")
print("=" * 50)

!pip install kagglehub mlflow scikit-learn xgboost lightgbm plotly seaborn -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç ML-–±–∏–±–ª–∏–æ—Ç–µ–∫
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
import xgboost as xgb
import lightgbm as lgb

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
import mlflow
import mlflow.sklearn
mlflow.set_experiment("ecommerce_demand_forecasting")

# –ß–∞—Å—Ç—å 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
import kagglehub
import pandas as pd
import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
from io import StringIO

print("=" * 50)
print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
print("=" * 50)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
path_date = kagglehub.dataset_download("mkechinov/ecommerce-behavior-data-from-multi-category-store")
print("–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤:", path_date)

# –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
files = list(Path(path_date).glob('*.csv'))
print("–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
for file in files:
    print(f"- {file.name}")

if files:
    # –í—ã–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª –∑–∞ –æ–∫—Ç—è–±—Ä—å 2019 –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è
    oct_file = next((f for f in files if '2019-Oct' in f.name), files[0])
    print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª: {oct_file.name}")

    # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    original_size_mb = os.path.getsize(oct_file) / 1024**2
    print(f"–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞: {original_size_mb:.2f} –ú–ë")

    # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    df_sample = pd.read_csv(oct_file, nrows=5)
    print("\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(df_sample.head())
    print("\n–ö–æ–ª–æ–Ω–∫–∏:", df_sample.columns.tolist())
    print("\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print(df_sample.info())

else:
    print("CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    exit()

# –ß–∞—Å—Ç—å 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
print("=" * 50)
print("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
print("=" * 50)

def optimize_data_loading(file_path, sample_fraction=0.05, chunk_size=50000):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏ –∏ –ø–æ—Ç–æ–∫–æ–≤—ã–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""

    print("–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö...")
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    sample_df = pd.read_csv(file_path, nrows=5000)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    dtype_optimized = {
        'event_time': 'object',
        'event_type': 'category',
        'product_id': 'int32',
        'category_id': 'int64',
        'category_code': 'category',
        'brand': 'category',
        'price': 'float32',
        'user_id': 'int32',
        'user_session': 'object'
    }

    # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    total_rows = 0
    with open(file_path, 'r') as f:
        total_rows = sum(1 for line in f) - 1  # –º–∏–Ω—É—Å –∑–∞–≥–æ–ª–æ–≤–æ–∫

    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ: {total_rows:,}")
    print(f"–¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —Å—ç–º–ø–ª–∞: {int(total_rows * sample_fraction):,} —Å—Ç—Ä–æ–∫")

    # –ü–æ—Ç–æ–∫–æ–≤–æ–µ —á—Ç–µ–Ω–∏–µ –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    sampled_chunks = []
    rows_processed = 0
    sample_every_n = int(1 / sample_fraction)

    print("–ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    for chunk in pd.read_csv(file_path, dtype=dtype_optimized, chunksize=chunk_size):
        # –°—ç–º–ø–ª–∏—Ä—É–µ–º –∏–∑ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        chunk_sample = chunk.sample(frac=sample_fraction, random_state=42)
        sampled_chunks.append(chunk_sample)

        rows_processed += len(chunk)
        progress = (rows_processed / total_rows) * 100
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {rows_processed:,} —Å—Ç—Ä–æ–∫ ({progress:.1f}%)")

        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        if len(pd.concat(sampled_chunks, ignore_index=True)) >= total_rows * sample_fraction * 1.1:
            break

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—ç–º–ø–ª—ã
    df = pd.concat(sampled_chunks, ignore_index=True)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if len(df) > total_rows * sample_fraction:
        df = df.sample(n=int(total_rows * sample_fraction), random_state=42)

    print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df):,} —Å—Ç—Ä–æ–∫")
    print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    return df

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
def load_limited_rows(file_path, n_rows=100000):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫"""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤—ã—Ö {n_rows:,} —Å—Ç—Ä–æ–∫...")

    dtype_optimized = {
        'event_time': 'object',
        'event_type': 'category',
        'product_id': 'int32',
        'category_id': 'int64',
        'category_code': 'category',
        'brand': 'category',
        'price': 'float32',
        'user_id': 'int32',
        'user_session': 'object'
    }

    df = pd.read_csv(file_path, dtype=dtype_optimized, nrows=n_rows)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å—Ç—Ä–æ–∫")
    print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    return df

# –ü—Ä–æ–±—É–µ–º –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞
try:
    print("–ü–æ–ø—ã—Ç–∫–∞ 1: –ü–æ—Ç–æ–∫–æ–≤–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (5% –¥–∞–Ω–Ω—ã—Ö)")
    df = optimize_data_loading(oct_file, sample_fraction=0.05)
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
    print("\n–ü–æ–ø—ã—Ç–∫–∞ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫")
    df = load_limited_rows(oct_file, n_rows=50000)

# –ß–∞—Å—Ç—å 4: –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ EDA
print("=" * 50)
print("–ü–ï–†–í–ò–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –ò EDA")
print("=" * 50)

print("1. –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–ù–ù–´–•:")
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
print(f"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {df['event_time'].min()} - {df['event_time'].max()}")

print("\n2. –¢–ò–ü–´ –î–ê–ù–ù–´–•:")
print(df.dtypes)

print("\n3. –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø:")
missing_data = df.isnull().sum()
missing_percent = (df.isnull().sum() / len(df)) * 100
missing_info = pd.DataFrame({
    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_data,
    '–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_percent
})
print(missing_info[missing_info['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤'] > 0])

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏
print("\n4. –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –í–†–ï–ú–ï–ù–ù–´–• –ú–ï–¢–û–ö...")
df['event_time'] = pd.to_datetime(df['event_time'])
df['date'] = df['event_time'].dt.date
df['day_of_week'] = df['event_time'].dt.dayofweek
df['hour'] = df['event_time'].dt.hour

print("\n5. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–û–ë–´–¢–ò–Ø–ú:")
event_counts = df['event_type'].value_counts()
print(event_counts)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
plt.figure(figsize=(15, 10))

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π
plt.subplot(2, 2, 1)
event_counts.plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen'])
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π')
plt.xlabel('–¢–∏–ø —Å–æ–±—ã—Ç–∏—è')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
plt.xticks(rotation=45)

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
plt.subplot(2, 2, 2)
df['day_of_week'].value_counts().sort_index().plot(kind='bar', color='lightblue')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏')
plt.xlabel('–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (0=–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫)')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π')

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–Ω (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)
plt.subplot(2, 2, 3)
prices = df['price'].dropna()
plt.hist(np.log1p(prices[prices > 0]), bins=50, alpha=0.7, color='orange')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–Ω (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)')
plt.xlabel('log(—Ü–µ–Ω–∞ + 1)')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')

# –¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤
plt.subplot(2, 2, 4)
top_brands = df['brand'].value_counts().head(10)
top_brands.plot(kind='bar', color='lightseagreen')
plt.title('–¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–±—ã—Ç–∏–π')
plt.xlabel('–ë—Ä–µ–Ω–¥')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# –ß–∞—Å—Ç—å 5: –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
print("=" * 50)
print("–ê–ù–ê–õ–ò–ó –í–†–ï–ú–ï–ù–ù–´–• –†–Ø–î–û–í –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
print("=" * 50)

# –ê–Ω–∞–ª–∏–∑ –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫
daily_purchases = df[df['event_type'] == 'purchase'].groupby('date').size()
daily_views = df[df['event_type'] == 'view'].groupby('date').size()

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
daily_purchases.plot(color='red', alpha=0.7)
plt.title('–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏')
plt.xlabel('–î–∞—Ç–∞')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫—É–ø–æ–∫')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
daily_views.plot(color='blue', alpha=0.7)
plt.title('–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã')
plt.xlabel('–î–∞—Ç–∞')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {df['product_id'].nunique()}")
print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {df['user_id'].nunique()}")
print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {df['category_id'].nunique()}")

# –ß–∞—Å—Ç—å 6: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø—Ä–æ—Å–∞
print("=" * 50)
print("–°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø")
print("=" * 50)

def create_demand_dataset(df, target_days=1):
    """–°–æ–∑–¥–∞–Ω–∏–µ dataset –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø—Ä–æ—Å–∞"""

    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é - –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–ø—Ä–æ—Å –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º
    daily_demand = df[df['event_type'] == 'purchase'].groupby(['date', 'product_id']).size().reset_index()
    daily_demand.columns = ['date', 'product_id', 'demand']

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features_list = []

    unique_dates = sorted(daily_demand['date'].unique())

    for i, current_date in enumerate(unique_dates[target_days:], target_days):
        # –¶–µ–ª–µ–≤–∞—è –¥–∞—Ç–∞
        target_date = unique_dates[i]
        # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        historical_start = unique_dates[i-target_days]
        historical_end = unique_dates[i-1]

        # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–Ω–∏
        historical_data = df[
            (df['date'] >= historical_start) &
            (df['date'] <= historical_end)
        ]

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–¥—É–∫—Ç–∞
        product_features = historical_data.groupby('product_id').agg({
            'event_type': [
                ('total_events', 'count'),
                ('views', lambda x: (x == 'view').sum()),
                ('cart_adds', lambda x: (x == 'cart').sum())
            ],
            'user_id': [
                ('unique_users', 'nunique')
            ],
            'price': [
                ('avg_price', 'mean'),
                ('price_std', 'std')
            ],
            'user_session': [
                ('unique_sessions', 'nunique')
            ]
        }).reset_index()

        # –£–ø—Ä–æ—â–∞–µ–º multi-index –∫–æ–ª–æ–Ω–∫–∏
        product_features.columns = ['product_id', 'total_events', 'views', 'cart_adds',
                                  'unique_users', 'avg_price', 'price_std', 'unique_sessions']

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
        product_features['target_date'] = target_date
        product_features['day_of_week'] = pd.to_datetime(target_date).dayofweek
        product_features['is_weekend'] = 1 if pd.to_datetime(target_date).dayofweek >= 5 else 0

        features_list.append(product_features)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features_df = pd.concat(features_list, ignore_index=True)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    final_df = pd.merge(
        features_df,
        daily_demand.rename(columns={'date': 'target_date'}),
        on=['target_date', 'product_id'],
        how='left'
    ).fillna(0)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –≥–¥–µ –Ω–µ –±—ã–ª–æ –ø–æ–∫—É–ø–æ–∫

    return final_df

print("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
demand_df = create_demand_dataset(df, target_days=1)

print(f"–†–∞–∑–º–µ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ dataset: {demand_df.shape}")
print("\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
print(demand_df.head())
print(f"\n–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'demand':")
print(f"Min: {demand_df['demand'].min()}, Max: {demand_df['demand'].max()}, Mean: {demand_df['demand'].mean():.2f}")

# –ß–∞—Å—Ç—å 7: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
print("=" * 50)
print("–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
print("=" * 50)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
X = demand_df.drop(['target_date', 'product_id', 'demand'], axis=1)
y = demand_df['demand']

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:")
print(X.isnull().sum())

# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
X = X.fillna(0)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, shuffle=True
)

print(f"–†–∞–∑–º–µ—Ä train: {X_train.shape}")
print(f"–†–∞–∑–º–µ—Ä test: {X_test.shape}")

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(n_estimators=50, random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(n_estimators=50, random_state=42, n_jobs=-1)
}

results = []

for name, model in models.items():
    print(f"\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {name}")

    with mlflow.start_run(run_name=name):
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        mlflow.log_param("model_type", name)
        mlflow.log_param("n_estimators", 50 if hasattr(model, 'n_estimators') else 'N/A')

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        start_time = datetime.now()
        model.fit(X_train, y_train)
        training_time = (datetime.now() - start_time).total_seconds()

        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
        y_pred = model.predict(X_test)

        # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫
        mape = mean_absolute_percentage_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = np.mean(np.abs(y_test - y_pred))

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        mlflow.log_metrics({
            "MAPE": mape,
            "RMSE": rmse,
            "MAE": mae,
            "training_time_seconds": training_time
        })

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        mlflow.sklearn.log_model(model, f"model_{name.lower().replace(' ', '_')}")

        results.append({
            'Model': name,
            'MAPE': mape,
            'RMSE': rmse,
            'MAE': mae,
            'Training Time (s)': training_time
        })

        print(f"  MAPE: {mape:.4f}, RMSE: {rmse:.4f}, Training Time: {training_time:.2f}s")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
results_df = pd.DataFrame(results)
print("\n" + "="*60)
print("–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ú–û–î–ï–õ–ï–ô")
print("="*60)
print(results_df.round(4))

# –ß–∞—Å—Ç—å 8: –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ (–ü–ï–†–ï–†–ê–ë–û–¢–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
print("=" * 50)
print("–°–û–ó–î–ê–ù–ò–ï –û–ü–¢–ò–ú–ê–õ–¨–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê")
print("=" * 50)

# –ò–º–ø–æ—Ä—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏
from sklearn.metrics import mean_absolute_error

# –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–º –≥–ª—É–±–æ–∫—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö
print("1. –ì–õ–£–ë–û–ö–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
print(f"–†–∞–∑–º–µ—Ä X_train: {X_train.shape}, X_test: {X_test.shape}")
print(f"y_train stats: min={y_train.min()}, max={y_train.max()}, mean={y_train.mean():.4f}, std={y_train.std():.4f}")
print(f"y_test stats: min={y_test.min()}, max={y_test.max()}, mean={y_test.mean():.4f}, std={y_test.std():.4f}")

# –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
zero_demand_count = (y_train == 0).sum()
zero_demand_percent = zero_demand_count / len(y_train) * 100
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ y_train: {zero_demand_count} ({zero_demand_percent:.2f}%)")

# –ê–Ω–∞–ª–∏–∑ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
non_zero_demand = y_train[y_train > 0]
print(f"–ù–µ–Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: min={non_zero_demand.min()}, max={non_zero_demand.max()}, mean={non_zero_demand.mean():.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(y_train, bins=50, alpha=0.7, color='blue', edgecolor='black', log=True)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ y_train (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)')
plt.xlabel('Demand')
plt.ylabel('Frequency (log)')

plt.subplot(1, 3, 2)
plt.boxplot(non_zero_demand)
plt.title('Boxplot –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π')
plt.ylabel('Demand')

plt.subplot(1, 3, 3)
# –ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 20 –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
y_sample = y_train[:20]
plt.bar(range(len(y_sample)), y_sample)
plt.title('–ü–µ—Ä–≤—ã–µ 20 –∑–Ω–∞—á–µ–Ω–∏–π y_train')
plt.xlabel('Index')
plt.ylabel('Demand')

plt.tight_layout()
plt.show()

print("\n2. –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú–´:")
print("–ü—Ä–æ–±–ª–µ–º–∞: 97.17% –∑–Ω–∞—á–µ–Ω–∏–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–≤–Ω—ã 0")
print("–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ —Å zero-inflated data")
print("–û–±—ã—á–Ω—ã–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö!")

# –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ MAPE
best_model_name = results_df.loc[results_df['MAPE'].idxmin(), 'Model']
print(f"\n3. –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ –ü–û –†–ï–ó–£–õ–¨–¢–ê–¢–ê–ú: {best_model_name}")

# –ü–ï–†–ï–†–ê–ë–û–¢–ê–ù–ù–´–ô –ü–û–î–•–û–î: –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è zero-inflated data
print("\n4. –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø –î–í–£–•–≠–¢–ê–ü–ù–û–ô –ú–û–î–ï–õ–ò:")

# –≠—Ç–∞–ø 1: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è - –±—É–¥–µ—Ç –ª–∏ —Å–ø—Ä–æ—Å > 0
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
y_train_binary = (y_train > 0).astype(int)
y_test_binary = (y_test > 0).astype(int)

print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
print(f"Train: 0s={np.sum(y_train_binary == 0)}, 1s={np.sum(y_train_binary == 1)}")
print(f"Test: 0s={np.sum(y_test_binary == 0)}, 1s={np.sum(y_test_binary == 1)}")

# –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
classifier_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))
])

print("–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞...")
classifier_pipeline.fit(X_train, y_train_binary)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
y_pred_binary = classifier_pipeline.predict(X_test)
y_pred_proba = classifier_pipeline.predict_proba(X_test)[:, 1]

print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
print(f"Accuracy: {accuracy_score(y_test_binary, y_pred_binary):.4f}")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_binary, y_pred_binary))

# –≠—Ç–∞–ø 2: –†–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
print("\n–û–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞ –¥–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")

# –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (—Ç–æ–ª—å–∫–æ –Ω–µ–Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
non_zero_mask = y_train > 0
X_train_non_zero = X_train[non_zero_mask]
y_train_non_zero = y_train[non_zero_mask]

print(f"–î–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: {len(X_train_non_zero)} —Å—Ç—Ä–æ–∫")

if len(X_train_non_zero) > 0:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏
    y_train_non_zero_log = np.log1p(y_train_non_zero)

    # –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    regressor_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('regressor', xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1
        ))
    ])

    regressor_pipeline.fit(X_train_non_zero, y_train_non_zero_log)

    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    y_pred_combined = np.zeros(len(X_test))

    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    for i in range(len(X_test)):
        if y_pred_binary[i] == 1:  # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ø—Ä–æ—Å
            features = X_test.iloc[i:i+1]  # –ë–µ—Ä–µ–º –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
            pred_log = regressor_pipeline.predict(features)[0]
            y_pred_combined[i] = np.expm1(pred_log)
        else:
            y_pred_combined[i] = 0

    # –û—Ü–µ–Ω–∫–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    final_mape = mean_absolute_percentage_error(y_test, y_pred_combined)
    final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_combined))
    final_mae = mean_absolute_error(y_test, y_pred_combined)

    # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –¥–ª—è zero-inflated data
    def zero_inflated_mape(y_true, y_pred):
        """MAPE –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç zero-inflated nature –¥–∞–Ω–Ω—ã—Ö"""
        # –î–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å—á–∏—Ç–∞–µ–º –æ–±—ã—á–Ω—ã–π MAPE
        non_zero_mask = y_true > 0
        if np.sum(non_zero_mask) > 0:
            mape_non_zero = mean_absolute_percentage_error(
                y_true[non_zero_mask],
                y_pred[non_zero_mask]
            )
        else:
            mape_non_zero = 0

        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—á–∏—Ç–∞–µ–º accuracy
        accuracy_binary = accuracy_score(
            (y_true > 0).astype(int),
            (y_pred > 0).astype(int)
        )

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
        combined_metric = 0.7 * mape_non_zero + 0.3 * (1 - accuracy_binary)
        return combined_metric, mape_non_zero, accuracy_binary

    custom_mape, mape_non_zero, accuracy_binary = zero_inflated_mape(y_test, y_pred_combined)

    print(f"\n5. –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–í–£–•–≠–¢–ê–ü–ù–û–ô –ú–û–î–ï–õ–ò:")
    print(f"   MAPE (–≤—Å–µ –¥–∞–Ω–Ω—ã–µ): {final_mape:.4f}")
    print(f"   MAPE (—Ç–æ–ª—å–∫–æ –Ω–µ–Ω—É–ª–µ–≤—ã–µ): {mape_non_zero:.4f}")
    print(f"   Accuracy (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è): {accuracy_binary:.4f}")
    print(f"   RMSE: {final_rmse:.4f}")
    print(f"   MAE: {final_mae:.4f}")
    print(f"   Custom MAPE: {custom_mape:.4f}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.scatter(y_test, y_pred_combined, alpha=0.5)
    max_val = max(y_test.max(), y_pred_combined.max())
    plt.plot([0, max_val], [0, max_val], 'red', linestyle='--')
    plt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    plt.title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è')

    plt.subplot(1, 3, 2)
    error = np.abs(y_test - y_pred_combined)
    plt.hist(error, bins=50, alpha=0.7, color='orange')
    plt.xlabel('–ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫')

    plt.subplot(1, 3, 3)
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    plt.hist(y_test, bins=50, alpha=0.5, label='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ', color='blue')
    plt.hist(y_pred_combined, bins=50, alpha=0.5, label='–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ', color='red')
    plt.xlabel('Demand')
    plt.ylabel('Frequency')
    plt.legend()
    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π')

    plt.tight_layout()
    plt.show()

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
    with mlflow.start_run(run_name="two_stage_model_final"):
        mlflow.log_params({
            "model_type": "Two-Stage (Classification + Regression)",
            "classifier": "RandomForest",
            "regressor": "XGBoost",
            "target_transformation": "log1p for non-zero values"
        })

        mlflow.log_metrics({
            "MAPE_all": final_mape,
            "MAPE_non_zero": mape_non_zero,
            "binary_accuracy": accuracy_binary,
            "RMSE": final_rmse,
            "MAE": final_mae,
            "custom_MAPE": custom_mape
        })

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–µ –º–æ–¥–µ–ª–∏
        mlflow.sklearn.log_model(classifier_pipeline, "classifier")
        mlflow.sklearn.log_model(regressor_pipeline, "regressor")

        print("\n6. –ü–†–û–í–ï–†–ö–ê –¢–†–ï–ë–û–í–ê–ù–ò–ô:")
        if final_mape < 0.15:
            print("‚úÖ –¢–†–ï–ë–û–í–ê–ù–ò–ï –í–´–ü–û–õ–ù–ï–ù–û: MAPE < 15%")
        else:
            print("‚ùå –¢–†–ï–ë–û–í–ê–ù–ò–ï –ù–ï –í–´–ü–û–õ–ù–ï–ù–û: MAPE >= 15%")
            print("   –û–¥–Ω–∞–∫–æ —É—á—Ç–∏—Ç–µ, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç 97% –Ω—É–ª–µ–π,")
            print("   —á—Ç–æ –¥–µ–ª–∞–µ—Ç –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ MAPE < 15% —á—Ä–µ–∑–≤—ã—á–∞–π–Ω–æ —Å–ª–æ–∂–Ω—ã–º.")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Å—É–¥–∏—Ç—å —Å –∑–∞–∫–∞–∑—á–∏–∫–æ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.")

else:
    print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞")

"""# –ß–∞—Å—Ç—å 2: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Feature Store"""

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Feast –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
print("=" * 50)
print("–£–°–¢–ê–ù–û–í–ö–ê FEAST –ò –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô")
print("=" * 50)

!pip install feast==0.31.0 pyarrow pandas scikit-learn -q

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
from pathlib import Path

# –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Feature Store
print("=" * 50)
print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø FEATURE STORE")
print("=" * 50)

# –°–æ–∑–¥–∞–¥–∏–º –±–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
def prepare_feature_store_data(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Feature Store"""

    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    df['event_timestamp'] = pd.to_datetime(df['event_time'])
    df['created_timestamp'] = datetime.now()

    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º –∏ –¥–Ω—è–º
    product_daily_features = df.groupby(['product_id', pd.Grouper(key='event_timestamp', freq='D')]).agg({
        'event_type': [
            ('total_views', lambda x: (x == 'view').sum()),
            ('total_carts', lambda x: (x == 'cart').sum()),
            ('total_purchases', lambda x: (x == 'purchase').sum())
        ],
        'user_id': [
            ('unique_users', 'nunique')
        ],
        'price': [
            ('avg_price', 'mean'),
            ('price_std', 'std'),
            ('min_price', 'min'),
            ('max_price', 'max')
        ],
        'user_session': [
            ('unique_sessions', 'nunique')
        ]
    }).reset_index()

    # –£–ø—Ä–æ—â–∞–µ–º multi-index –∫–æ–ª–æ–Ω–∫–∏
    product_daily_features.columns = ['product_id', 'event_timestamp',
                                    'total_views', 'total_carts', 'total_purchases',
                                    'unique_users', 'avg_price', 'price_std',
                                    'min_price', 'max_price', 'unique_sessions']

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    product_daily_features = product_daily_features.fillna(0)

    # –î–æ–±–∞–≤–ª—è–µ–º created_timestamp
    product_daily_features['created_timestamp'] = datetime.now()

    # –î–æ–±–∞–≤–ª—è–µ–º –ª–∞–≥–æ–≤—ã–µ —Ñ–∏—á–∏ (–ø—Ä–æ–¥–∞–∂–∏ –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–µ–Ω—å)
    product_daily_features = product_daily_features.sort_values(['product_id', 'event_timestamp'])
    product_daily_features['prev_day_purchases'] = product_daily_features.groupby('product_id')['total_purchases'].shift(1)
    product_daily_features['prev_day_views'] = product_daily_features.groupby('product_id')['total_views'].shift(1)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ª–∞–≥–æ–≤—ã—Ö —Ñ–∏—á–∞—Ö
    product_daily_features[['prev_day_purchases', 'prev_day_views']] = product_daily_features[['prev_day_purchases', 'prev_day_views']].fillna(0)

    return product_daily_features

# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á...")
feature_data = prepare_feature_store_data(df)

print(f"–†–∞–∑–º–µ—Ä –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {feature_data.shape}")
print("\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
print(feature_data.head())
print(f"\n–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {feature_data['event_timestamp'].min()} - {feature_data['event_timestamp'].max()}")

# –ß–∞—Å—Ç—å 2: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ Feast
print("=" * 50)
print("–°–û–ó–î–ê–ù–ò–ï –ü–†–û–ï–ö–¢–ê FEAST")
print("=" * 50)

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Feast
feast_repo_path = "demand_forecasting_feast"
os.makedirs(feast_repo_path, exist_ok=True)

# –°–æ–∑–¥–∞–µ–º feature_store.yaml
feature_store_yaml = f"""
project: demand_forecasting
registry: {feast_repo_path}/registry.db
provider: local
online_store:
    path: {feast_repo_path}/online_store.db
entity_key_serialization_version: 2
"""

with open(f"{feast_repo_path}/feature_store.yaml", "w") as f:
    f.write(feature_store_yaml)

print(f"–°–æ–∑–¥–∞–Ω –ø—Ä–æ–µ–∫—Ç Feast –≤: {feast_repo_path}")

# –ß–∞—Å—Ç—å 3: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏—á –≤ features.py
print("=" * 50)
print("–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –§–ò–ß –í FEAST")
print("=" * 50)

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(f"{feast_repo_path}/data", exist_ok=True)
print(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {feast_repo_path}/data")

# –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª features.py —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
features_py_content = '''from datetime import timedelta
from feast import Entity, Feature, FeatureView, ValueType
from feast.data_source import FileSource

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ - –ø—Ä–æ–¥—É–∫—Ç
product = Entity(
    name="product_id",
    value_type=ValueType.INT64,
    description="–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–¥—É–∫—Ç–∞",
)

# –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞
product_stats_source = FileSource(
    path="{path}",  # –ø—É—Ç—å –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω –ø–æ–∑–∂–µ
    event_timestamp_column="event_timestamp",
    created_timestamp_column="created_timestamp",
)

# Feature View –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
product_daily_stats = FeatureView(
    name="product_daily_stats",
    entities=["product_id"],
    ttl=timedelta(days=7),  # –¥–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã 7 –¥–Ω–µ–π
    features=[
        Feature(name="total_views", dtype=ValueType.INT64),
        Feature(name="total_carts", dtype=ValueType.INT64),
        Feature(name="total_purchases", dtype=ValueType.INT64),
        Feature(name="unique_users", dtype=ValueType.INT64),
        Feature(name="avg_price", dtype=ValueType.DOUBLE),
        Feature(name="price_std", dtype=ValueType.DOUBLE),
        Feature(name="min_price", dtype=ValueType.DOUBLE),
        Feature(name="max_price", dtype=ValueType.DOUBLE),
        Feature(name="unique_sessions", dtype=ValueType.INT64),
        Feature(name="prev_day_purchases", dtype=ValueType.INT64),
        Feature(name="prev_day_views", dtype=ValueType.INT64),
    ],
    online=True,
    batch_source=product_stats_source,
    tags={{"team": "demand_forecasting", "type": "daily_stats"}},
)

# Feature View –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è)
product_3day_stats = FeatureView(
    name="product_3day_stats",
    entities=["product_id"],
    ttl=timedelta(days=10),
    features=[
        Feature(name="views_3d_avg", dtype=ValueType.DOUBLE),
        Feature(name="purchases_3d_avg", dtype=ValueType.DOUBLE),
        Feature(name="conversion_rate_3d", dtype=ValueType.DOUBLE),
    ],
    online=True,
    batch_source=product_stats_source,  # –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –±—ã–ª –±—ã –æ—Ç–¥–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
    tags={{"team": "demand_forecasting", "type": "aggregated_stats"}},
)
'''

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ–∏—á –≤ Parquet —Ñ–∞–π–ª
feature_data_path = f"{feast_repo_path}/data/product_daily_stats.parquet"
print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤: {feature_data_path}")

try:
    feature_data.to_parquet(feature_data_path, index=False)
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Ñ–∏—á —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {feature_data_path}")
    print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(feature_data_path) / 1024**2:.2f} MB")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
    # –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
    demo_data = pd.DataFrame({
        'product_id': [1, 2, 3],
        'event_timestamp': [datetime.now() - timedelta(days=i) for i in range(3)],
        'created_timestamp': [datetime.now()] * 3,
        'total_views': [100, 150, 200],
        'total_purchases': [10, 15, 20],
        'avg_price': [25.5, 30.0, 35.5]
    })
    demo_data.to_parquet(feature_data_path, index=False)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤: {feature_data_path}")

# –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –≤ features.py —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–º–µ–Ω—É —Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞
    features_py_content_final = features_py_content.replace("{path}", feature_data_path)

    with open(f"{feast_repo_path}/features.py", "w") as f:
        f.write(features_py_content_final)

    print("‚úÖ –§–∞–π–ª features.py —Å–æ–∑–¥–∞–Ω")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ features.py: {e}")
    # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± - –∑–∞–ø–∏—Å–∞—Ç—å —Ñ–∞–π–ª –±–µ–∑ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    with open(f"{feast_repo_path}/features.py", "w") as f:
        f.write(f'''
from datetime import timedelta
from feast import Entity, Feature, FeatureView, ValueType
from feast.data_source import FileSource

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ - –ø—Ä–æ–¥—É–∫—Ç
product = Entity(
    name="product_id",
    value_type=ValueType.INT64,
    description="–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–¥—É–∫—Ç–∞",
)

# –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞
product_stats_source = FileSource(
    path="{feature_data_path}",
    event_timestamp_column="event_timestamp",
    created_timestamp_column="created_timestamp",
)

# Feature View –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
product_daily_stats = FeatureView(
    name="product_daily_stats",
    entities=["product_id"],
    ttl=timedelta(days=7),
    features=[
        Feature(name="total_views", dtype=ValueType.INT64),
        Feature(name="total_carts", dtype=ValueType.INT64),
        Feature(name="total_purchases", dtype=ValueType.INT64),
        Feature(name="unique_users", dtype=ValueType.INT64),
        Feature(name="avg_price", dtype=ValueType.DOUBLE),
        Feature(name="price_std", dtype=ValueType.DOUBLE),
        Feature(name="min_price", dtype=ValueType.DOUBLE),
        Feature(name="max_price", dtype=ValueType.DOUBLE),
        Feature(name="unique_sessions", dtype=ValueType.INT64),
        Feature(name="prev_day_purchases", dtype=ValueType.INT64),
        Feature(name="prev_day_views", dtype=ValueType.INT64),
    ],
    online=True,
    batch_source=product_stats_source,
    tags={{"team": "demand_forecasting", "type": "daily_stats"}},
)

# Feature View –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
product_3day_stats = FeatureView(
    name="product_3day_stats",
    entities=["product_id"],
    ttl=timedelta(days=10),
    features=[
        Feature(name="views_3d_avg", dtype=ValueType.DOUBLE),
        Feature(name="purchases_3d_avg", dtype=ValueType.DOUBLE),
        Feature(name="conversion_rate_3d", dtype=ValueType.DOUBLE),
    ],
    online=True,
    batch_source=product_stats_source,
    tags={{"team": "demand_forecasting", "type": "aggregated_stats"}},
)
''')
    print("‚úÖ –§–∞–π–ª features.py —Å–æ–∑–¥–∞–Ω (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã
print("\nüìÅ –ü–†–û–í–ï–†–ö–ê –°–û–ó–î–ê–ù–ù–´–• –§–ê–ô–õ–û–í:")
created_files = [
    f"{feast_repo_path}/feature_store.yaml",
    f"{feast_repo_path}/features.py",
    f"{feast_repo_path}/data/product_daily_stats.parquet"
]

for file_path in created_files:
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path) / 1024  # KB
        print(f"‚úÖ {file_path} ({file_size:.1f} KB)")
    else:
        print(f"‚ùå {file_path} - –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —á–∏—Ç–∞–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ features.py
print("\nüìù –°–û–î–ï–†–ñ–ò–ú–û–ï features.py:")
try:
    with open(f"{feast_repo_path}/features.py", "r") as f:
        content = f.read()
        print("–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ñ–∞–π–ª–∞:")
        lines = content.split('\n')[:10]
        for i, line in enumerate(lines, 1):
            print(f"{i:2d}: {line}")
except Exception as e:
    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å features.py: {e}")

# –ß–∞—Å—Ç—å 4: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Feature Store
print("=" * 50)
print("–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø FEATURE STORE")
print("=" * 50)

try:
    from feast import FeatureStore

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Feature Store
    store = FeatureStore(repo_path=feast_repo_path)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∏—á
    print("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ñ–∏—á...")

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ features.py
    import importlib.util
    import sys

    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º features.py
    spec = importlib.util.spec_from_file_location("features", f"{feast_repo_path}/features.py")
    features_module = importlib.util.module_from_spec(spec)
    sys.modules["features"] = features_module
    spec.loader.exec_module(features_module)

    # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –∏–∑ –º–æ–¥—É–ª—è
    product = features_module.product
    product_daily_stats = features_module.product_daily_stats
    product_3day_stats = features_module.product_3day_stats

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏—á–∏
    store.apply([product, product_daily_stats, product_3day_stats])

    print("‚úÖ Feature Store –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ Feature Views: {[fv.name for fv in store.list_feature_views()]}")
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ Entities: {[e.name for e in store.list_entities()]}")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Feature Store: {e}")
    print("–°–æ–∑–¥–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π Feature Store...")

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è —Å–ª—É—á–∞—è –æ—à–∏–±–∫–∏
    class DemoFeatureStore:
        def __init__(self):
            self.feature_views = ["product_daily_stats", "product_3day_stats"]
            self.entities = ["product_id"]

        def list_feature_views(self):
            class FeatureView:
                def __init__(self, name):
                    self.name = name
            return [FeatureView(fv) for fv in self.feature_views]

        def list_entities(self):
            class Entity:
                def __init__(self, name):
                    self.name = name
            return [Entity(e) for e in self.entities]

        def get_historical_features(self, entity_df, features):
            class Result:
                def to_df(self):
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    demo_df = entity_df.copy()
                    for feature in features:
                        feature_name = feature.split(":")[1] if ":" in feature else feature
                        if "views" in feature_name:
                            demo_df[feature_name] = np.random.randint(50, 200, len(entity_df))
                        elif "purchases" in feature_name:
                            demo_df[feature_name] = np.random.randint(0, 10, len(entity_df))
                        elif "price" in feature_name:
                            demo_df[feature_name] = np.random.uniform(20, 100, len(entity_df))
                        else:
                            demo_df[feature_name] = np.random.randint(10, 50, len(entity_df))
                    return demo_df
            return Result()

        def get_online_features(self, features, entity_rows):
            class Result:
                def to_dict(self):
                    result = {}
                    for feature in features:
                        feature_name = feature.split(":")[1] if ":" in feature else feature
                        result[feature_name] = [np.random.randint(50, 200)]
                    result['product_id'] = [entity_rows[0]['product_id']]
                    return result
            return Result()

    store = DemoFeatureStore()
    print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π Feature Store —Å–æ–∑–¥–∞–Ω")

"""# –í–ê–†–ò–ê–ù–¢ 2"""

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyArrow –æ—Ç–¥–µ–ª—å–Ω–æ, –ø–µ—Ä–µ–¥ —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π Feast
!pip install "pyarrow>=14.0.0"

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ Feast
print("=" * 50)
print("–£–°–¢–ê–ù–û–í–ö–ê FEAST")
print("=" * 50)

# –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä—Å–∏–π, —Ç–∞–∫ –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
try:
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
    !pip install feast==0.31.0 -q
    print("‚úÖ Feast 0.31.0 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ Feast 0.31.0: {e}")
    try:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è
        !pip install feast==0.40.1 -q
        print("‚úÖ Feast 0.40.1 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
    except Exception as e2:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ Feast 0.40.1: {e2}")
        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
        !pip install feast -q
        print("‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è Feast")

# –ü—Ä–æ–≤–µ—Ä–∏–º —É—Å—Ç–∞–Ω–æ–≤–∫—É
try:
    import feast
    print(f"‚úÖ Feast —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω, –≤–µ—Ä—Å–∏—è: {feast.__version__}")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ Feast: {e}")

"""# –ß–∞—Å—Ç—å 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MLflow"""



"""# –ß–∞—Å—Ç—å 4: –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –≤ production"""



"""# –ß–∞—Å—Ç—å 5: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π"""



"""# –ß–∞—Å—Ç—å 6: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–±—É—á–µ–Ω–∏—è"""



"""# –ß–∞—Å—Ç—å 7: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"""

